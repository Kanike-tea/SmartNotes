25.3 Hadoop Distributed File System (HDFS)

monitors this queue and instructs a DataNode to create replicas and distribute
them across racks. NameNode prefers to have as many different racks as possible to
host replicas of a block. Overreplicated blocks cause some replicas to be removed
based on space utilization of the DataNodes.

25.3.4 HDFS Scalability

Since we are discussing big data technologies in this chapter, it is apropos to discuss
some limits of scalability in HDFS. Hadoop program management committee
member Shvachko commented that the Yahoo HDFS cluster had achieved the fol-
lowing levels as opposed to the intended targets (Shvachko, 2010). The numbers in
parentheses are the targets he listed. Capacity: 14 petabytes (vs. 10 petabytes); num-
ber of nodes: 4,000 (vs. 10,000); clients:15,000 (vs. 100,000); and files: 60 million
(vs. 100 million). Thus, Yahoo had come very close to its intended targets in 2010,
with a smaller cluster of 4,000 nodes and fewer clients; but Yahoo had actually
exceeded the target with respect to total amount of data handled.

Some of the observations made by Shvachko (2010) are worth mentioning. They
are based on the HDFS configuration used at Yahoo in 2010. We present the actual
and estimated numbers below to give the reader a sense of what is involved in these
gigantic data processing environments.

= The blocksize used was 128K, and an average file contained 1.5 blocks.
NameNode used about 200 bytes per block and an additional 200 bytes for
an i-node. 100 million files referencing 200 million blocks would require
RAM capacity exceeding 60 GB.

= For 100 million files with size of 200 million blocks and a replication factor
of 3, the disk space required is 60 PB. Thus a rule of thumb was proposed
that 1 GB of RAM in NameNode roughly corresponds to 1 PB of data stor-
age based on the assumption of 128K blocksize and 1.5 blocks per file.

= In order to hold 60 PB of data on a 10,000-node cluster, each node needs a
capacity of 6 TB. This can be achieved by having eight 0.75-TB drives.

= The internal workload for the NameNode is block reports. About 3 reports
per second containing block information on 60K blocks per report were
received by the NameNode.

= The external load on the NameNode consisted of external connections and
tasks from MapReduce jobs. This resulted in tens of thousands of simultane-
ous connections.

= The Client Read consisted of performing a block lookup to get block loca-
tions from the NameNode, followed by accessing the nearest replica of the
block. A typical client (the Map job from an MR task) would read data from
1,000 files with an average reading of half a file each, amounting to 96 MB of
data. This was estimated to take 1.45 seconds. At that rate, 100,000 clients
would send 68,750 block-location requests per second to the NameNode.
This was considered to be well within the capacity of the NameNode, which
was rated at handling 126K requests per second.

925