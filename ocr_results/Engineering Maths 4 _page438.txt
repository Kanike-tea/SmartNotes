. Probability ° Pi;

conclude that the transition ma

- <1, A person commutes the distance to his
“he does not go by train for two consecutive

” —— 429
STOCHASTIC: PROCESS _—T

[5.22] Markov Chains

A stochastic process which is such that 'the generation of the probability distribution
edepend only on the present state is called a Markov process. ;
If this state space is discrete ( finite or countably infinite) we say that the process is a

discrete state process oi chain. Then the Markov process is known as aMarkov chain.
arOF s

Further if the state space is continuous, the Process is called a continuous state process,

We explicitly define a Markov chain as follows. .

Let the outcomes X,, X,, .:.0fa sequence of trials satisfy the following properties.

(i)’ Each outcome. belong to the finite set (state space) of the outcomes
a, Gay oy} : ; a 8
(ii) The outcome of

any trial depend at most upon the outcome of the immediate
preceeding trial. ° ot HF

is associated ‘with every pair of states (4, 4;) that 4; occurs

immediately after 4; occurs. Such a_ stochastic process is called a

finite Markoo chain These probabilities (2; ; ) which arenon zero real numbers are called

transition probabilities and they form a square matrix of order m called the transition
probability matrix (t.p.m) denoted by P.

Pr Py2 ++ Pim

ie, P=[nj]= 21 raid am

With. each: state a; there. corresponds the i®

tow of transition probabilities
Pits Piar +++Pi yy It is evident that the elements of

P have the following properties,
m
@) Os Py S 1(ii) > Pi =1
i .

(i= 1,2,3,...m)

The above two properties satisfy the requirement of a stochastic matrix and hence we

trix of a Markov chain is q stochastic matrix,
ting t.p.m of a Markoy chain

Office everyday either by train or by bus. Suppose:
f days, but if he goes by bus the next day he is just as
likely to go by bus again ashe is to travel by train: -

Illustrative Examples for wri